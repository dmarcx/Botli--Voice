<!DOCTYPE html>
<html lang="he">
<head>
  <meta charset="UTF-8">
  <title>Botli â€“ ×”××–× ×” ×¨×¦×™×¤×” ×•×ª××œ×•×œ ××•×˜×•××˜×™</title>
  <style>
    body { font-family: sans-serif; direction: rtl; text-align: center; padding: 40px; }
    h1 { font-size: 28px; }
    #status, #transcription { font-size: 20px; margin-top: 20px; }
    button { margin-top: 30px; padding: 10px 20px; font-size: 16px; }
  </style>
</head>
<body>
  <h1>ğŸ¤ Botli â€“ ×”××–× ×” ×§×•×œ×™×ª ×¨×¦×™×¤×” ×œ-GPT</h1>
  <div id="status">×××–×™×Ÿ...</div>
  <div id="transcription">×ª×•×¦××•×ª ×™×•×¤×™×• ×›××Ÿ ×•×™×¤×•×¨×¡××•</div>
  <button onclick="downloadTranscripts()">ğŸ“¥ ×”×•×¨×“ ×ª××œ×•×œ</button>

  <script>
    let mediaRecorder;
    let audioChunks = [];
    let isRecording = false;
    let recentRMS = [];
    let lastRMS = 0;
    let recordStartTime = 0;
    let transcriptLog = [];
    let silenceCounter = 0;
    let silenceThreshold = 0.01;
    let requiredSilenceFrames = 25; // 25 x 20ms â‰ˆ 500ms
    let minRecordingDuration = 2500;

    function downloadTranscripts() {
      const content = transcriptLog.map(entry => `${entry.time}: ${entry.text}`).join("\n");
      const blob = new Blob([content], { type: 'text/plain;charset=utf-8' });
      const link = document.createElement('a');
      link.href = URL.createObjectURL(blob);
      link.download = 'transcripts.txt';
      link.click();
    }

    async function initContinuousRecording() {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const context = new AudioContext();
      const source = context.createMediaStreamSource(stream);
      const processor = context.createScriptProcessor(2048, 1, 1);

      source.connect(processor);
      processor.connect(context.destination);

      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) audioChunks.push(e.data);
      };

      mediaRecorder.onstop = async () => {
        const recordingDuration = Date.now() - recordStartTime;
        const blob = new Blob(audioChunks, { type: 'audio/webm' });
        audioChunks = [];

        if (blob.size < 2000 || recordingDuration < minRecordingDuration) {
          document.getElementById("transcription").textContent = "ğŸ“ ×˜×§×¡×˜ ××–×•×”: (×©×§×˜ ××• ×§×¦×¨ ××“×™)";
          if (isRecording) startListening();
          return;
        }

        const averageRMS = recentRMS.reduce((a, b) => a + b, 0) / recentRMS.length;
        lastRMS = averageRMS;
        recentRMS = [];

        document.getElementById('status').textContent = "â¬› ×©×•×œ×— ×œ-Whisper...";

        const formData = new FormData();
        formData.append("file", blob, "audio.webm");

        try {
          const res = await fetch("http://localhost:5000/transcribe", {
            method: "POST",
            body: formData
          });
          const data = await res.json();
          let text = (data.text || '').trim();

          const genericResponses = ["×ª×•×“×”", "×ª×•×“×” ×¨×‘×”", "×©×œ×•×", "×©×œ×•× ×œ×š", "Thank you", "Thanks"];
          text = text.normalize("NFKC");
          const normalized = text.replace(/[.,!?Ö¾\s\u200f\u200e]/g, '').toLowerCase();
          const cleanNormalized = normalized.replace(/[^Ö-×¿a-zA-Z0-9]/gu, '');
          const allowedShortWords = ['×›×Ÿ', '×œ×', '××”', '××™', '××™'];

          const isGenericExact = genericResponses.includes(text);
          const isProbablySilence = (
            !text ||
            cleanNormalized.length === 0 ||
            ((isGenericExact || cleanNormalized.startsWith("×ª×•×“×”")) && lastRMS < 0.005 && cleanNormalized.length <= 10 && !allowedShortWords.includes(cleanNormalized)) ||
            (cleanNormalized.length < 3 && !allowedShortWords.includes(cleanNormalized))
          );

          if (!isProbablySilence) {
            transcriptLog.push({ time: new Date().toISOString(), text });
          }

          document.getElementById("transcription").textContent = isProbablySilence
            ? "ğŸ“ ×˜×§×¡×˜ ××–×•×”: (×©×§×˜ ××• ×œ× ××–×•×”)"
            : "ğŸ“ ×˜×§×¡×˜ ××–×•×”: " + text;
        } catch (e) {
          document.getElementById("transcription").textContent = '[×©×’×™××” ×‘×©×œ×™×—×” ×œ×©×¨×ª]';
        }

        isRecording = false;
        startListening();
      };

      processor.onaudioprocess = e => {
        const input = e.inputBuffer.getChannelData(0);
        const rms = Math.sqrt(input.reduce((sum, val) => sum + val * val, 0) / input.length);
        recentRMS.push(rms);
        if (recentRMS.length > 30) recentRMS.shift();

        const isSilent = rms < silenceThreshold;

        if (!isSilent && !isRecording) {
          document.getElementById("status").textContent = "ğŸ”´ ××§×œ×™×˜...";
          isRecording = true;
          recordStartTime = Date.now();
          if (mediaRecorder.state !== "recording") mediaRecorder.start();
        }

        if (isRecording && isSilent) {
          silenceCounter++;
          if (silenceCounter > requiredSilenceFrames) {
            document.getElementById("status").textContent = "â¬› ×¢×•×¦×¨ ×”×§×œ×˜×”...";
            isRecording = false;
            silenceCounter = 0;
            if (mediaRecorder.state === "recording") mediaRecorder.stop();
          }
        } else if (!isSilent) {
          silenceCounter = 0;
        }
      };
    }

    function startListening() {
      document.getElementById("status").textContent = "×××–×™×Ÿ... (×“×‘×¨ ×‘×¨×¦×£)";
      if (!isRecording && mediaRecorder.state !== "recording") {
        recordStartTime = 0;
        mediaRecorder.start();
        isRecording = true;
        document.getElementById("status").textContent = "ğŸ”´ ××§×œ×™×˜...";
      }
    }

    initContinuousRecording();
  </script>
</body>
</html>
