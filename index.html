<!DOCTYPE html>
<html lang="he">
<head>
  <meta charset="UTF-8">
  <title>Botli â€“ ×”××–× ×” ×¨×¦×™×¤×” ×•×ª××œ×•×œ ××•×˜×•××˜×™</title>
  <style>
    body { font-family: sans-serif; direction: rtl; text-align: center; padding: 40px; }
    h1 { font-size: 28px; }
    #status, #transcription { font-size: 20px; margin-top: 20px; }
  </style>
</head>
<body>
  <h1>ğŸ¤ Botli â€“ ×”××–× ×” ×§×•×œ×™×ª ×¨×¦×™×¤×” ×œ-GPT</h1>
  <div id="status">×××–×™×Ÿ...</div>
  <div id="transcription">×ª×•×¦××•×ª ×™×•×¤×™×• ×›××Ÿ ×•×™×¤×•×¨×¡××•</div>

  <script>
    let mediaRecorder;
    let audioChunks = [];
    let isRecording = false;

    async function initContinuousRecording() {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const context = new AudioContext();
      const source = context.createMediaStreamSource(stream);
      const processor = context.createScriptProcessor(2048, 1, 1);

      source.connect(processor);
      processor.connect(context.destination);

      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) audioChunks.push(e.data);
      };

      mediaRecorder.onstop = async () => {
        const blob = new Blob(audioChunks, { type: 'audio/webm' });
        audioChunks = [];

        if (blob.size < 2000) {
          document.getElementById("transcription").textContent = "ğŸ“ ×˜×§×¡×˜ ××–×•×”×”: (×©×§×˜ ××• ×œ× ××–×•×”×”)";
          if (isRecording) startListening();
          return;
        }

        document.getElementById('status').textContent = "â¬› ×©×•×œ×— ×œ-Whisper...";

        const formData = new FormData();
        formData.append("file", blob, "audio.webm");

        try {
          const res = await fetch("http://localhost:5000/transcribe", {
            method: "POST",
            body: formData
          });
          const data = await res.json();
          const text = (data.text || '').trim();

          const genericResponses = ["×ª×•×“×”", "×ª×•×“×” ×¨×‘×”", "×©×œ×•×", "×©×œ×•× ×œ×š"];
          if (!text || genericResponses.includes(text)) {
            document.getElementById("transcription").textContent = "ğŸ“ ×˜×§×¡×˜ ××–×•×”×”: (×©×§×˜ ××• ×œ× ××–×•×”×”)";
          } else {
            document.getElementById("transcription").textContent = "ğŸ“ ×˜×§×¡×˜ ××–×•×”×”: " + text;
          }
        } catch (e) {
          document.getElementById("transcription").textContent = '[×©×’×™××” ×‘×©×œ×™×—×” ×œ×©×¨×ª]';
        }

        if (isRecording) {
          startListening();
        }
      };

      let silenceCounter = 0;
      processor.onaudioprocess = e => {
        const input = e.inputBuffer.getChannelData(0);
        const isSilent = input.every(sample => Math.abs(sample) < 0.01);

        if (!isSilent && !isRecording) {
          document.getElementById("status").textContent = "ğŸ”´ ××§×œ×™×˜...";
          isRecording = true;
          mediaRecorder.start();
        }

        if (isRecording && isSilent) {
          silenceCounter++;
          if (silenceCounter > 20) {
            document.getElementById("status").textContent = "â¬› ×¢×•×¦×¨ ×”×§×œ×˜×”...";
            isRecording = false;
            silenceCounter = 0;
            mediaRecorder.stop();
          }
        } else if (!isSilent) {
          silenceCounter = 0;
        }
      };
    }

    function startListening() {
      document.getElementById("status").textContent = "×××–×™×Ÿ... (×“×‘×¨ ×¨×¦×™×¤×•×ª)";
    }

    initContinuousRecording();
  </script>
</body>
</html>
