<!DOCTYPE html>
<html lang="he" dir="rtl">
<head>
  <meta charset="UTF-8">
  <title>Botli Voice GPT</title>
  <style>
    body { font-family: sans-serif; text-align: center; padding: 2em; direction: rtl; }
    #startBtn { font-size: 1.5em; padding: 0.5em 2em; }
    #result { margin-top: 2em; font-size: 1.2em; color: #333; }
    #volumeMeter {
      width: 300px;
      height: 20px;
      background: #eee;
      margin: 1em auto;
      border-radius: 10px;
      overflow: hidden;
    }
    #volumeLevel {
      height: 100%;
      width: 0%;
      background: green;
    }
  </style>
</head>
<body>
  <h1>Botli ğŸ—£ï¸ ××–×”×” ×§×•×œ</h1>
  <button id="startBtn">ğŸ™ï¸ ×”×ª×—×œ ×”××–× ×”</button>
  <div id="volumeMeter"><div id="volumeLevel"></div></div>
  <div id="result">××—×›×” ×œ×§×œ×˜ ×§×•×œ×™...</div>

  <script>
    const startBtn = document.getElementById('startBtn');
    const resultDiv = document.getElementById('result');
    const volumeLevel = document.getElementById('volumeLevel');

    let audioContext;
    let micStream;
    let analyser;
    let dataArray;
    let speechRecognition;
    let vadThreshold = 20;
    let silenceTimeout;

    function updateVolumeMeter() {
      if (!analyser) return;

      analyser.getByteFrequencyData(dataArray);
      let volume = dataArray.reduce((a, b) => a + b) / dataArray.length;
      volumeLevel.style.width = Math.min(volume, 100) + '%';

      if (volume > vadThreshold) {
        startRecognition();
        clearTimeout(silenceTimeout);
      } else {
        silenceTimeout = setTimeout(() => {
          stopRecognition();
        }, 2500);
      }

      requestAnimationFrame(updateVolumeMeter);
    }

    function startAudioProcessing() {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
        micStream = audioContext.createMediaStreamSource(stream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 512;
        dataArray = new Uint8Array(analyser.frequencyBinCount);
        micStream.connect(analyser);
        updateVolumeMeter();
      }).catch(err => {
        console.error('ğŸ¤ ×œ× ×”×¦×œ×—× ×• ×œ×’×©×ª ×œ××™×§×¨×•×¤×•×Ÿ:', err);
      });
    }

    function startRecognition() {
      if (speechRecognition && speechRecognition.recognizing) return;
      speechRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      speechRecognition.lang = 'he-IL';
      speechRecognition.interimResults = false;

      speechRecognition.onresult = event => {
        const text = event.results[0][0].transcript;
        resultDiv.textContent = 'ğŸ”Š ×–×™×”×•×™: ' + text;
      };

      speechRecognition.onend = () => {
        speechRecognition.recognizing = false;
      };

      speechRecognition.start();
      speechRecognition.recognizing = true;
    }

    function stopRecognition() {
      if (speechRecognition && speechRecognition.recognizing) {
        speechRecognition.stop();
        speechRecognition.recognizing = false;
      }
    }

    startBtn.onclick = () => {
      resultDiv.textContent = 'ğŸ§ ×××–×™×Ÿ...';
      startAudioProcessing();
    };
  </script>
</body>
</html>
