<!DOCTYPE html>
<html lang="he">
<head>
  <meta charset="UTF-8">
  <title>Botli Whisper-GPT-TTS</title>
  <style>
    body { font-family: sans-serif; text-align: center; direction: rtl; padding: 2em; }
    button { font-size: 1.2em; padding: 10px 20px; margin: 10px; }
    #status { font-size: 1.3em; margin: 20px; }
    #transcript { font-size: 1.4em; margin-top: 20px; color: #333; }
    #wave { width: 300px; height: 10px; background: #eee; margin: 0 auto; }
    #level { height: 100%; width: 0%; background: green; }
  </style>
</head>
<body>
  <h1>ğŸ™ï¸ Botli â€“ ×©×œ×™×—×” ×§×•×œ×™×ª ×œ-GPT</h1>
  <button id="recordBtn">ğŸ¤ ×”×ª×—×œ ×”×§×œ×˜×”</button>
  <div id="status">××•×›×Ÿ...</div>
  <div id="wave"><div id="level"></div></div>
  <div id="transcript">...</div>

  <script>
    const recordBtn = document.getElementById("recordBtn");
    const status = document.getElementById("status");
    const transcriptDiv = document.getElementById("transcript");
    const levelBar = document.getElementById("level");

    let mediaRecorder;
    let audioChunks = [];
    let audioContext;
    let analyser;
    let source;
    let rafId;

    function visualizeMic(stream) {
      audioContext = new AudioContext();
      source = audioContext.createMediaStreamSource(stream);
      analyser = audioContext.createAnalyser();
      source.connect(analyser);
      const dataArray = new Uint8Array(analyser.fftSize);

      function draw() {
        analyser.getByteTimeDomainData(dataArray);
        const max = Math.max(...dataArray);
        const percent = ((max - 128) / 128) * 100;
        levelBar.style.width = Math.min(Math.abs(percent), 100) + "%";
        rafId = requestAnimationFrame(draw);
      }
      draw();
    }

    async function sendToWhisper(blob) {
      status.textContent = "ğŸ§ ×©×•×œ×— ×œ-Whisper...";
      const formData = new FormData();
      formData.append("audio", blob, "audio.webm");

      const res = await fetch("http://localhost:5000/transcribe", {
        method: "POST",
        body: formData
      });

      const data = await res.json();
      const text = data.text;
      transcriptDiv.textContent = "ğŸ“œ ×˜×§×¡×˜ ××–×•×”×”: " + text;

      status.textContent = "ğŸ“¨ ×©×•×œ×— ×œ-GPT...";
      const gptRes = await fetch("http://localhost:3080/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ prompt: text })
      });

      const gptData = await gptRes.json();
      const reply = gptData.response;
      status.textContent = "ğŸ¤– GPT ×¢× ×”: " + reply;
      speak(reply);
    }

    function speak(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "he-IL";
      speechSynthesis.speak(utterance);
    }

    recordBtn.onclick = async () => {
      if (mediaRecorder && mediaRecorder.state === "recording") {
        mediaRecorder.stop();
        status.textContent = "ğŸ›‘ ×¢×•×¦×¨ ×”×§×œ×˜×”...";
        cancelAnimationFrame(rafId);
        return;
      }

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      visualizeMic(stream);

      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = (e) => {
        audioChunks.push(e.data);
      };

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
        sendToWhisper(audioBlob);
      };

      mediaRecorder.start();
      status.textContent = "ğŸ™ï¸ ××§×œ×™×˜... ×œ×—×¥ ×©×•×‘ ×œ×¢×¦×™×¨×”";
    };
  </script>
</body>
</html>
